{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import csv\n",
    "import sys\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "sys.path.append('../src') \n",
    "\n",
    "from dataset_utils import Static_dataGen, Dynamic_dataGen, Bonus_dataGen, Preprocessing\n",
    "\n",
    "key = os.environ.get(\"OPEN_AI_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static Dataset Creation\n",
    "Aim: To generate a set of query-output pairs using the original set of 9 tools\n",
    "\n",
    "Method: \n",
    "1. A set of 3-4 tools is sampled every iteration for query generation\n",
    "2. The sampled set of tools is passed to an LLM agent for query generation\n",
    "3. The query is then passed to another agent, along with their descriptions, to generate its completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m staticDatagen \u001b[38;5;241m=\u001b[39m \u001b[43mStatic_dataGen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m no_of_StaticQuery_CompletionPairs2beGen \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      5\u001b[0m data_dict \u001b[38;5;241m=\u001b[39m staticDatagen\u001b[38;5;241m.\u001b[39mgenQuery(no_of_StaticQuery_CompletionPairs2beGen)\n",
      "File \u001b[0;32m~/inter-iit/Final_Code/notebooks/../src/dataset_utils.py:16\u001b[0m, in \u001b[0;36mStatic_dataGen.__init__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputCompletion \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mno_of_Queries2beGenerated \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/devrev-git/lib/python3.12/site-packages/openai/_client.py:93\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m     91\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m     )\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "staticDatagen = Static_dataGen(key)\n",
    "\n",
    "no_of_StaticQuery_CompletionPairs2beGen = 10\n",
    "\n",
    "data_dict = staticDatagen.genQuery(no_of_StaticQuery_CompletionPairs2beGen)\n",
    "\n",
    "if not os.path.exists('../datasets/Generated/raw_data'):\n",
    "    os.makedirs('../datasets/Generated/raw_data')\n",
    "\n",
    "field_names= ['Query','Output']\n",
    "\n",
    "with open('../datasets/Generated/raw_data/saveStaticdataset.csv', 'w') as csv_file:  \n",
    "    csv_writer = csv.DictWriter(csv_file, fieldnames=data_dict[0].keys())\n",
    "    csv_writer.writeheader()\n",
    "    csv_writer.writerows(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Dataset Creation\n",
    "Aim: To generate a dynamic toolset, and combining them with the original toolset to obtain a set of query-output pairs\n",
    "\n",
    "Method (Dynamic Toolset Creation): \n",
    "1. 4 tools are sampled from the original toolset every iteration\n",
    "2. These tools are then passed to an agent, to generate similar tools\n",
    "\n",
    "Method (Query-Output Pair Generation): \n",
    "1. Random 10 tools along with the original 9 at a time are passed to the agent for generating queries. The model has the liberty to select any number of tools from this for query generation. \n",
    "2. Another agent then generates the completions for the query list\n",
    "(The query list is cleaned by code and manual intervention before passing to the second agent, and a similar process is followed for the final CSV creation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamicDatagen = Dynamic_dataGen(key)\n",
    "\n",
    "no_of_newTool2beAdded = 10\n",
    "\n",
    "no_of_DynamicQuery_CompletionPairs2beGen = 10\n",
    "\n",
    "if not os.path.exists('../datasets/Generated/raw_data'):\n",
    "    os.makedirs('../datasets/Generated/raw_data')\n",
    "\n",
    "dynamicDatagen.genDynamicTools(no_of_newTool2beAdded)\n",
    "\n",
    "data_dict = dynamicDatagen.genDynamicQueryOutputPair(no_of_DynamicQuery_CompletionPairs2beGen)\n",
    "\n",
    "field_names= ['Added_Tools','Query','Output']\n",
    "\n",
    "with open('../datasets/Generated/raw_data/saveDynamicData.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Dataset Creation\n",
    "Aim: To generate a set of query-output pairs which involves usage of conditional and iterative operators\n",
    "\n",
    "Method: Manually creating a list of 5 such query-output pairs, feeding these examples along with a list of a few relevant dynamic tools combined with the original toolset in the query-generating agent, and finally passing this list of queries in the completion agent. At every step of output from the model, the data is cleaned before saving and passing to the further agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonusDatagen = Bonus_dataGen(key)\n",
    "\n",
    "no_of_BonusQuery_CompletionPairs2beGen = 10\n",
    "\n",
    "if not os.path.exists('../datasets/Generated/raw_data'):\n",
    "    os.makedirs('../datasets/Generated/raw_data')\n",
    "\n",
    "data_dict = bonusDatagen.genBonusQueryOutputPair(no_of_BonusQuery_CompletionPairs2beGen)\n",
    "\n",
    "field_names= ['Query','Output']\n",
    "\n",
    "with open('../datasets/Generated/raw_data/saveBonusData.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restructuring Dataset For Different Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset formation for P1 Pipeline\n",
    "\n",
    "Since the P1 pipeline does not require a training set, the following code generates an evaluation dataset for the P1 pipeline. The docstring is created by choosing the tools used in the query along with some random tools from the tools list. Since the data has to be used for infering the model, and has no prior knowledge of the tools, it requires the docstring of the allowed tools, along with some examples (few-shot) in the prompt to generate good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_df = pd.read_csv(\"../datasets/Pre-Generated/raw_data/static_dataset.csv\") \n",
    "dynamic_df = pd.read_csv(\"../datasets/Pre-Generated/raw_data/dynamic_dataset.csv\") \n",
    "bonus_df = pd.read_csv(\"../datasets/Pre-Generated/raw_data/bonus_dataset.csv\", encoding= 'unicode_escape') \n",
    "bonusTool_list = [row[0] for row in csv.reader(open('../resources/Tool_list/final-bonus-toolset.csv', 'r'))]\n",
    "\n",
    "datasetForm = Preprocessing()\n",
    "\n",
    "if not os.path.exists('../datasets/Generated/P1_datasets/test'):\n",
    "    os.makedirs('../datasets/Generated/P1_datasets/test')\n",
    "\n",
    "#Static\n",
    "staticDictP1 = []\n",
    "\n",
    "for i, row in static_df.iterrows():\n",
    "    query = row['Query']\n",
    "    output = row['Output']\n",
    "    added_tools = datasetForm.p1_static()\n",
    "    prompt = datasetForm.prompt_p1_static_dynamic(query, added_tools)\n",
    "    staticDictP1.append({'Query': prompt,'Output' : output})\n",
    "\n",
    "field_names= ['Query', 'Output']\n",
    "\n",
    "P1_static_test = staticDictP1[0:round(0.1*len(staticDictP1))]\n",
    "\n",
    "with open('../datasets/Generated/P1_datasets/test/StaticP1dataset_test.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(P1_static_test)\n",
    "\n",
    "#Dynamic\n",
    "dynamicDictP1 = []\n",
    "\n",
    "for i, row in dynamic_df.iterrows():\n",
    "    query = row['Query']\n",
    "    output = row['Output']\n",
    "    additional_tools = ast.literal_eval(row['Added_Tools'].replace(\"['\", \"['''\").replace(\"']\", \"''']\"))\n",
    "    added_tools = datasetForm.p1_dynamic(additional_tools)\n",
    "    prompt = datasetForm.prompt_p1_static_dynamic(query, added_tools)\n",
    "    dynamicDictP1.append({'Query': prompt,'Output' : output})\n",
    "\n",
    "field_names= ['Query', 'Output']\n",
    "\n",
    "P1_dynamic_test = dynamicDictP1[0:round(0.33*len(dynamicDictP1))]\n",
    "\n",
    "\n",
    "with open('../datasets/Generated/P1_datasets/test/DynamicP1dataset_test.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(P1_dynamic_test)\n",
    "\n",
    "# Bonus\n",
    "bonusDictP1 = []\n",
    "for i, row in bonus_df.iterrows():\n",
    "    query = row['Query']\n",
    "    output = row['Output']\n",
    "    added_tools = datasetForm.p1_bonus(bonusTool_list)\n",
    "    prompt = datasetForm.prompt_p1_bonus(query, added_tools)\n",
    "    bonusDictP1.append({'Query': prompt,'Output' : output})\n",
    "\n",
    "field_names= ['Query', 'Output']\n",
    "\n",
    "P1_bonus_test = bonusDictP1[0:round(0.1*len(bonusDictP1))]\n",
    "\n",
    "with open('../datasets/Generated/P1_datasets/test/BonusP1dataset_test.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(P1_bonus_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt formation for P2 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_df = pd.read_csv(\"../datasets/Pre-Generated/raw_data/static_dataset.csv\") \n",
    "dynamic_df = pd.read_csv(\"../datasets/Pre-Generated/raw_data/dynamic_dataset.csv\") \n",
    "bonus_df = pd.read_csv(\"../datasets/Pre-Generated/raw_data/bonus_dataset.csv\", encoding= 'unicode_escape') \n",
    "bonusTool_list = [row[0] for row in csv.reader(open('../resources/Tool_list/final-bonus-toolset.csv', 'r'))]\n",
    "\n",
    "datasetForm = Preprocessing()\n",
    "\n",
    "if not os.path.exists('../datasets/Generated/P2_datasets/train_val'):\n",
    "    os.makedirs('../datasets/Generated/P2_datasets/train_val')\n",
    "\n",
    "if not os.path.exists('../datasets/Generated/P2_datasets/test'):\n",
    "    os.makedirs('../datasets/Generated/P2_datasets/test')\n",
    "\n",
    "#Static\n",
    "staticDictP2 = []\n",
    "for i, row in static_df.iterrows():\n",
    "    query = row['Query']\n",
    "    output = row['Output']\n",
    "    prompt = datasetForm.prompt_p2_pipeline(query,output)\n",
    "    staticDictP2.append({'Prompt':prompt, 'Output' : output})\n",
    "\n",
    "field_names= ['Prompt', 'Output']\n",
    "\n",
    "P2_test_static = staticDictP2[0:round(0.1*len(staticDictP2))]\n",
    "\n",
    "with open('../datasets/Generated/P2_datasets/test/StaticP2prompt_test.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(P2_test_static)\n",
    "\n",
    "#Dynamic\n",
    "dynamicDictP2 = []\n",
    "for i, row in dynamic_df.iterrows():\n",
    "    query = row['Query']\n",
    "    output = row['Output']\n",
    "    additional_tools = ast.literal_eval(row['Added_Tools'].replace(\"['\", \"['''\").replace(\"']\", \"''']\"))\n",
    "    prompt = datasetForm.prompt_p2_pipeline(query,output,additional_tools)\n",
    "    dynamicDictP2.append({'Prompt':prompt, 'Output' : output})\n",
    "\n",
    "field_names= ['Prompt', 'Output']\n",
    "\n",
    "P2_test_dynamic = dynamicDictP2[0:round(0.33*len(dynamicDictP2))]\n",
    "\n",
    "with open('../datasets/Generated/P2_datasets/test/DynamicP2prompt_test.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(P2_test_dynamic)\n",
    "\n",
    "#Bonus\n",
    "bonusDictP2 = []\n",
    "for i, row in bonus_df.iterrows():\n",
    "    query = row['Query']\n",
    "    output = row['Output']\n",
    "    prompt = datasetForm.prompt_p2_pipeline(query,output,bonusTool_list)\n",
    "    bonusDictP2.append({'Prompt':prompt, 'Output' : output})\n",
    "\n",
    "field_names= ['Prompt', 'Output']\n",
    "\n",
    "P2_test_bonus = bonusDictP2[0:round(0.1*len(bonusDictP2))]\n",
    "\n",
    "with open('../datasets/Generated/P2_datasets/test/BonusP2prompt_test.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(P2_test_bonus)\n",
    "\n",
    "P2_train_val = staticDictP2[round(0.1*len(staticDictP2)):] + dynamicDictP2[round(0.33*len(dynamicDictP2)):] + bonusDictP2[round(0.9*len(bonusDictP2)):]\n",
    "random.shuffle(P2_train_val)\n",
    "\n",
    "P2_val = P2_train_val[0:round(0.1*len(P2_train_val))]\n",
    "P2_train = P2_train_val[round(0.1*len(P2_train_val)):]\n",
    "\n",
    "\n",
    "with open('../datasets/Generated/P2_datasets/train_val/P2prompt_train.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(P2_train)\n",
    "\n",
    "with open('../datasets/Generated/P2_datasets/train_val/P2prompt_val.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(P2_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt formation for P3 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_df = pd.read_csv(\"../datasets/Pre-Generated/raw_data/static_dataset.csv\") \n",
    "dynamic_df = pd.read_csv(\"../datasets/Pre-Generated/raw_data/dynamic_dataset.csv\") \n",
    "bonus_df = pd.read_csv(\"../datasets/Pre-Generated/raw_data/bonus_dataset.csv\", encoding= 'unicode_escape') \n",
    "bonusTool_list = [row[0] for row in csv.reader(open('../resources/Tool_list/final-bonus-toolset.csv', 'r'))]\n",
    "\n",
    "datasetForm = Preprocessing()\n",
    "\n",
    "if not os.path.exists('../datasets/Generated/P3_datasets/train_val'):\n",
    "    os.makedirs('../datasets/Generated/P3_datasets/train_val')\n",
    "\n",
    "if not os.path.exists('../datasets/Generated/P3_datasets/test'):\n",
    "    os.makedirs('../datasets/Generated/P3_datasets/test')\n",
    "\n",
    "#Static\n",
    "staticDictP3 = []\n",
    "for i, row in static_df.iterrows():\n",
    "    query = row['Query']\n",
    "    output = row['Output']\n",
    "    prompt = datasetForm.prompt_p3_pipeline(query,output)\n",
    "    staticDictP3.append({'Prompt':prompt, 'Output' : output})\n",
    "\n",
    "field_names= ['Prompt', 'Output']\n",
    "\n",
    "P3_test_static = staticDictP3[0:round(0.1*len(staticDictP3))]\n",
    "\n",
    "with open('../datasets/Generated/P3_datasets/test/StaticP3prompt_test.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(P3_test_static)\n",
    "\n",
    "#Dynamic\n",
    "dynamicDictP3 = []\n",
    "for i, row in dynamic_df.iterrows():\n",
    "    query = row['Query']\n",
    "    output = row['Output']\n",
    "    additional_tools = ast.literal_eval(row['Added_Tools'].replace(\"['\", \"['''\").replace(\"']\", \"''']\"))\n",
    "    prompt = datasetForm.prompt_p3_pipeline(query,output,additional_tools)\n",
    "    dynamicDictP3.append({'Prompt':prompt, 'Output' : output})\n",
    "\n",
    "field_names= ['Prompt', 'Output']\n",
    "\n",
    "P3_test_dynamic = dynamicDictP3[0:round(0.33*len(dynamicDictP3))]\n",
    "\n",
    "with open('../datasets/Generated/P3_datasets/test/DynamicP3prompt_test.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(P3_test_dynamic)\n",
    "\n",
    "#Bonus\n",
    "bonusDictP3 = []\n",
    "for i, row in bonus_df.iterrows():\n",
    "    query = row['Query']\n",
    "    output = row['Output']\n",
    "    prompt = datasetForm.prompt_p3_pipeline(query,output,bonusTool_list)\n",
    "    bonusDictP3.append({'Prompt':prompt, 'Output' : output})\n",
    "\n",
    "field_names= ['Prompt', 'Output']\n",
    "\n",
    "P3_test_bonus = bonusDictP3[0:round(0.1*len(bonusDictP3))]\n",
    "\n",
    "with open('../datasets/Generated/P3_datasets/test/BonusP3prompt_test.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(P3_test_bonus)\n",
    "\n",
    "P3_train_val_stage_1 = staticDictP3[round(0.1*len(staticDictP3)):]\n",
    "P3_train_val_stage_2 = staticDictP3[round(0.75*len(staticDictP3)):] + bonusDictP3\n",
    "\n",
    "random.shuffle(P3_train_val_stage_1)\n",
    "random.shuffle(P3_train_val_stage_2)\n",
    "\n",
    "\n",
    "P3_val_stage_1 = P3_train_val_stage_1[0:round(0.1*len(P3_train_val_stage_1))]\n",
    "P3_train_stage_1 = P3_train_val_stage_1[round(0.1*len(P3_train_val_stage_1)):]\n",
    "\n",
    "P3_val_stage_2 = P3_train_val_stage_2[0:round(0.1*len(P3_train_val_stage_2))]\n",
    "P3_train_stage_2 = P3_train_val_stage_2[round(0.1*len(P3_train_val_stage_2)):]\n",
    "\n",
    "if not os.path.exists('../datasets/Generated/P3_datasets/train_val/Stage-1'):\n",
    "    os.makedirs('../datasets/Generated/P3_datasets/train_val/Stage-1')\n",
    "\n",
    "if not os.path.exists('../datasets/Generated/P3_datasets/train_val/Stage-2'):\n",
    "    os.makedirs('../datasets/Generated/P3_datasets/train_val/Stage-2')\n",
    "\n",
    "with open('../datasets/Generated/P3_datasets/train_val/Stage-1/P3prompt_stage_1_train.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(P3_train_stage_1)\n",
    "\n",
    "with open('../datasets/Generated/P3_datasets/train_val/Stage-1/P3prompt_stage_1_val.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(P3_val_stage_1)\n",
    "\n",
    "with open('../datasets/Generated/P3_datasets/train_val/Stage-2/P3prompt_stage_2_train.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(P3_train_stage_2)\n",
    "\n",
    "with open('../datasets/Generated/P3_datasets/train_val/Stage-2/P3prompt_stage_2_val.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(P3_val_stage_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-rev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
