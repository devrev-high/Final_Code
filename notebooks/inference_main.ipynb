{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('../src')\n",
    "\n",
    "from inference_utils import P1_inferecening, P2_P3_inferencing\n",
    "from evaluator import evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P1 Pipeline Inferencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference for Static Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_df = pd.read_csv(\"../datasets/Pre-Generated/P1_datasets/test/StaticP1dataset_test.csv\")\n",
    "model = \"deepseek-ai/deepseek-coder-1.3b-instruct\"\n",
    "\n",
    "if not os.path.exists('../output/P1_infered_output'):\n",
    "    os.makedirs('../output/P1_infered_output')\n",
    "\n",
    "static_output = []\n",
    "P1_inferencing_for_Static = P1_inferecening()\n",
    "for i, row in static_df.iterrows():\n",
    "    static_dict = {'query': row['Prompt'],\n",
    "                   'output': row['Output']}\n",
    "    ans, latency = P1_inferencing_for_Static.completionP1StaticDynamic(model,static_dict)\n",
    "    static_output.append({'Output': ans , 'Ground_Truth': row['Output'],'Latency': latency})\n",
    "\n",
    "field_names= ['Output','Ground_Truth','Latency']\n",
    "\n",
    "with open('../output/P1_infered_output/staticInference.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(static_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference for Dynamic Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_df = pd.read_csv(\"../datasets/Pre-Generated/P1_datasets/test/DynamicP1dataset_test.csv\")\n",
    "model_name = \"deepseek-ai/deepseek-coder-1.3b-instruct\"\n",
    "\n",
    "if not os.path.exists('../output/P1_infered_output'):\n",
    "    os.makedirs('../output/P1_infered_output')\n",
    "\n",
    "dynamic_output = []\n",
    "P1_inferencing_for_dynamic =P1_inferecening()\n",
    "for i, row in dynamic_df.iterrows():\n",
    "    dynamic_dict = {'query': row['Prompt'],\n",
    "                   'output': row['Output']}\n",
    "    ans, latency = P1_inferencing_for_dynamic.completionP1StaticDynamic(model, dynamic_dict)\n",
    "    dynamic_output.append({'Output': ans , 'Ground_Truth': row['Output'],'Latency': latency})\n",
    "\n",
    "field_names= ['Output','Ground_Truth','Latency']\n",
    "\n",
    "with open('../output/P1_infered_output/dynamicInference.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(dynamic_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference for Bonus Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonus_df = pd.read_csv(\"../datasets/Pre-Generated/P1_datasets/test/BonusP1dataset_test.csv\")\n",
    "model_name = \"deepseek-ai/deepseek-coder-1.3b-instruct\"\n",
    "\n",
    "if not os.path.exists('../output/P1_infered_output'):\n",
    "    os.makedirs('../output/P1_infered_output')\n",
    "\n",
    "bonus_output = []\n",
    "P1_inferencing_for_bonus = P1_inferecening()\n",
    "for i, row in bonus_df.iterrows():\n",
    "    bonus_dict = {'query': row['Prompt'],\n",
    "                   'output': row['Output']}\n",
    "    ans, latency = P1_inferencing_for_bonus.completionP1Bonus(model_name,bonus_dict)\n",
    "    bonus_output.append({'Output': ans , 'Ground_Truth': row['Output'],'Latency': latency})\n",
    "\n",
    "field_names= ['Output','Ground_Truth','Latency']\n",
    "\n",
    "with open('../output/P1_infered_output/bonusInference.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(bonus_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference for P2 Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For static dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_df = pd.read_csv(\"../datasets/Pre-Generated/P2_datasets/test/StaticP2prompt_test.csv\")\n",
    "\n",
    "model_name = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "infer_model = \"Insight244/codellama-p3-data-no-dynamic-no-random-no-bonus\"\n",
    "\n",
    "if not os.path.exists('../output/P2_infered_output'):\n",
    "    os.makedirs('../output/P2_infered_output')\n",
    "\n",
    "P2_inferencing_for_Static = P2_P3_inferencing()\n",
    "P2_inferencing_for_Static.P2_P3_load_model(model_name, infer_model)\n",
    "\n",
    "static_output = []\n",
    "\n",
    "for i, row in static_df.iterrows():\n",
    "    static_dict = {'query': row['Prompt'],\n",
    "                   'output': row['Output']}\n",
    "    \n",
    "    ans, latency = P2_inferencing_for_Static.P2_P3_get_inference(static_dict)\n",
    "\n",
    "    static_output.append({'Output': ans , 'Ground_Truth': row['Output'],'Latency': latency})\n",
    "\n",
    "field_names= ['Output','Ground_Truth','Latency']\n",
    "\n",
    "with open('../output/P2_infered_output/staticInference.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(static_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for dynamic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_df = pd.read_csv(\"../datasets/Pre-Generated/P2_datasets/test/DynamicP2prompt_test.csv\")\n",
    "\n",
    "model_name = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "infer_model = \"Insight244/codellama-p3-data-no-dynamic-no-random-no-bonus\"\n",
    "\n",
    "if not os.path.exists('../output/P2_infered_output'):\n",
    "    os.makedirs('../output/P2_infered_output')\n",
    "\n",
    "P2_inferencing_for_dynamic = P2_P3_inferencing()\n",
    "P2_inferencing_for_dynamic.P2_P3_load_model(model_name, infer_model)\n",
    "\n",
    "dynamic_output = []\n",
    "\n",
    "for i, row in dynamic_df.iterrows():\n",
    "    dynamic_dict = {'query': row['Query'],\n",
    "                   'output': row['Output'],}\n",
    "\n",
    "    ans, latency = P2_inferencing_for_dynamic.P2_P3_get_inference(dynamic_dict)\n",
    "\n",
    "    dynamic_output.append({'Output': ans , 'Ground_Truth': row['Output'],'Latency': latency})\n",
    "\n",
    "field_names= ['Output','Ground_Truth','Latency']\n",
    "\n",
    "with open('../output/P2_infered_output/dynamicInference.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(dynamic_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for bonus dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonus_df = pd.read_csv(\"../datasets/Pre-Generated/P2_datasets/test/BonusP2prompt_test.csv\")\n",
    "\n",
    "model_name = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "infer_model = \"Insight244/codellama-p3-data-no-dynamic-no-random-no-bonus\"\n",
    "\n",
    "if not os.path.exists('../output/P2_infered_output'):\n",
    "    os.makedirs('../output/P2_infered_output')\n",
    "\n",
    "P2_inferencing_for_bonus = P2_P3_inferencing()\n",
    "P2_inferencing_for_bonus.P2_P3_load_model(model_name, infer_model)\n",
    "\n",
    "bonus_output = []\n",
    "\n",
    "for i, row in bonus_df.iterrows():\n",
    "    bonus_dict = {'query': row['Query'],\n",
    "                   'output': row['Output'],}\n",
    "\n",
    "    ans, latency = P2_inferencing_for_bonus.P2_P3_get_inference(bonus_dict)\n",
    "\n",
    "    bonus_output.append({'Output': ans , 'Ground_Truth': row['Output'],'Latency': latency})\n",
    "\n",
    "field_names= ['Output','Ground_Truth','Latency']\n",
    "\n",
    "with open('../output/P2_infered_output/bonusInference.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(bonus_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference for P3 Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For static dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_df = pd.read_csv(\"../datasets/Pre-Generated/P3_datasets/test/StaticP3prompt_test.csv\")\n",
    "\n",
    "model_name = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "infer_model = \"Insight244/codellama-p3-data-no-dynamic-no-random-no-bonus\"\n",
    "\n",
    "if not os.path.exists('../output/P3_infered_output'):\n",
    "    os.makedirs('../output/P3_infered_output')\n",
    "\n",
    "P3_inferencing_for_Static = P2_P3_inferencing()\n",
    "P3_inferencing_for_Static.P2_P3_load_model(model_name, infer_model)\n",
    "\n",
    "static_output = []\n",
    "\n",
    "for i, row in static_df.iterrows():\n",
    "    static_dict = {'query': row['Query'],\n",
    "                   'output': row['Output'],}\n",
    "\n",
    "    ans, latency = P3_inferencing_for_Static.P2_P3_get_inference(static_dict)\n",
    "\n",
    "    static_output.append({'Output': ans , 'Ground_Truth': row['Output'],'Latency': latency})\n",
    "\n",
    "field_names= ['Output','Ground_Truth','Latency']\n",
    "\n",
    "with open('../output/P3_infered_output/staticInference.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(static_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for dynamic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_df = pd.read_csv(\"../datasets/Pre-Generated/P3_datasets/test/DynamicP3prompt_test.csv\")\n",
    "\n",
    "model_name = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "infer_model = \"Insight244/codellama-p3-data-no-dynamic-no-random-no-bonus\"\n",
    "\n",
    "if not os.path.exists('../output/P3_infered_output'):\n",
    "    os.makedirs('../output/P3_infered_output')\n",
    "\n",
    "P3_inferencing_for_dynamic = P2_P3_inferencing()\n",
    "P3_inferencing_for_dynamic.P2_P3_load_model(model_name, infer_model)\n",
    "\n",
    "dynamic_output = []\n",
    "\n",
    "for i, row in dynamic_df.iterrows():\n",
    "    dynamic_dict = {'query': row['Query'],\n",
    "                   'output': row['Output'],}\n",
    "\n",
    "    ans, latency = P3_inferencing_for_dynamic.P2_P3_get_inference(dynamic_dict)\n",
    "\n",
    "    dynamic_output.append({'Output': ans , 'Ground_Truth': row['Output'],'Latency': latency})\n",
    "\n",
    "field_names= ['Output','Ground_Truth','Latency']\n",
    "\n",
    "with open('../output/P3_infered_output/dynamicInference.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(dynamic_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for bonus dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonus_df = pd.read_csv(\"../datasets/Pre-Generated/P3_datasets/test/BonusP3prompt_test.csv\")\n",
    "\n",
    "model_name = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "infer_model = \"Insight244/codellama-p3-data-no-dynamic-no-random-no-bonus\"\n",
    "\n",
    "if not os.path.exists('../output/P3_infered_output'):\n",
    "    os.makedirs('../output/P3_infered_output')\n",
    "\n",
    "P3_inferencing_for_bonus = P2_P3_inferencing()\n",
    "P3_inferencing_for_bonus.P2_P3_load_model(model_name, infer_model)\n",
    "\n",
    "bonus_output = []\n",
    "\n",
    "for i, row in bonus_df.iterrows():\n",
    "    bonus_dict = {'query': row['Query'],\n",
    "                   'output': row['Output']}\n",
    "\n",
    "    ans, latency = P3_inferencing_for_bonus.P2_P3_get_inference(bonus_dict)\n",
    "\n",
    "    bonus_output.append({'Output': ans , 'Ground_Truth': row['Output'],'Latency': latency})\n",
    "\n",
    "field_names= ['Output','Ground_Truth','Latency']\n",
    "\n",
    "with open('../output/P3_infered_output/bonusInference.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(bonus_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of P1 pipeline on Static Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv(\"../output/P1_infered_output/staticInference.csv\")\n",
    "\n",
    "precision, recall, F1, langeval, latency = evaluator(eval_df)\n",
    "\n",
    "\n",
    "print(\"Evaluation of P1 pipeline on Static Dataset\")\n",
    "print(\"Precision = \", precision)\n",
    "print(\"Recall = \", recall)\n",
    "print(\"F1 = \", F1)\n",
    "print(\"LangChain Metric = \", langeval)\n",
    "print(\"Latency = \" , latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of P1 pipeline on Dynamic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv(\"../output/P1_infered_output/dynamicInference.csv\")\n",
    "\n",
    "precision, recall, F1, langeval, latency = evaluator(eval_df)\n",
    "\n",
    "\n",
    "print(\"Evaluation of P1 pipeline on Dynamic Dataset\")\n",
    "print(\"Precision = \", precision)\n",
    "print(\"Recall = \", recall)\n",
    "print(\"F1 = \", F1)\n",
    "print(\"LangChain Metric = \", langeval)\n",
    "print(\"Latency = \" , latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of P1 pipeline on Bonus Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv(\"../output/P1_infered_output/bonusInference.csv\")\n",
    "\n",
    "precision, recall, F1, langeval, latency = evaluator(eval_df)\n",
    "\n",
    "print(\"Evaluation of P1 pipeline on Dynamic Dataset\")\n",
    "print(\"Precision = \", precision)\n",
    "print(\"Recall = \", recall)\n",
    "print(\"F1 = \", F1)\n",
    "print(\"LangChain Metric = \", langeval)\n",
    "print(\"Latency = \" , latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of P2 pipeline on Static Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv(\"../output/P2_infered_output/staticInference.csv\")\n",
    "\n",
    "precision, recall, F1, langeval, latency = evaluator(eval_df)\n",
    "\n",
    "print(\"Evaluation of P2 pipeline on Static Dataset\")\n",
    "print(\"Precision = \", precision)\n",
    "print(\"Recall = \", recall)\n",
    "print(\"F1 = \", F1)\n",
    "print(\"LangChain Metric = \", langeval)\n",
    "print(\"Latency = \" , latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of P2 pipeline on Dynamic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv(\"../output/P2_infered_output/dynamicInference.csv\")\n",
    "\n",
    "precision, recall, F1, langeval, latency = evaluator(eval_df)\n",
    "\n",
    "print(\"Evaluation of P2 pipeline on Dynamic Dataset\")\n",
    "print(\"Precision = \", precision)\n",
    "print(\"Recall = \", recall)\n",
    "print(\"F1 = \", F1)\n",
    "print(\"LangChain Metric = \", langeval)\n",
    "print(\"Latency = \" , latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of P2 pipeline on Bonus Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv(\"../output/P2_infered_output/bonusInference.csv\")\n",
    "\n",
    "precision, recall, F1, langeval, latency = evaluator(eval_df)\n",
    "\n",
    "print(\"Evaluation of P2 pipeline on Bonus Dataset\")\n",
    "print(\"Precision = \", precision)\n",
    "print(\"Recall = \", recall)\n",
    "print(\"F1 = \", F1)\n",
    "print(\"LangChain Metric = \", langeval)\n",
    "print(\"Latency = \" , latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of P3 pipeline on Static Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv(\"../output/P3_infered_output/staticInference.csv\")\n",
    "\n",
    "precision, recall, F1, langeval, latency = evaluator(eval_df)\n",
    "\n",
    "print(\"Evaluation of P3 pipeline on Static Dataset\")\n",
    "print(\"Precision = \", precision)\n",
    "print(\"Recall = \", recall)\n",
    "print(\"F1 = \", F1)\n",
    "print(\"LangChain Metric = \", langeval)\n",
    "print(\"Latency = \" , latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of P3 pipeline on Dynamic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv(\"../output/P3_infered_output/dynamicInference.csv\")\n",
    "\n",
    "precision, recall, F1, langeval, latency = evaluator(eval_df)\n",
    "\n",
    "print(\"Evaluation of P3 pipeline on Dynamic Dataset\")\n",
    "print(\"Precision = \", precision)\n",
    "print(\"Recall = \", recall)\n",
    "print(\"F1 = \", F1)\n",
    "print(\"LangChain Metric = \", langeval)\n",
    "print(\"Latency = \" , latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of P3 pipeline on Bonus Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv(\"../output/P3_infered_output/bonusInference.csv\")\n",
    "\n",
    "precision, recall, F1, langeval, latency = evaluator(eval_df)\n",
    "\n",
    "print(\"Evaluation of P3 pipeline on Bonus Dataset\")\n",
    "print(\"Precision = \", precision)\n",
    "print(\"Recall = \", recall)\n",
    "print(\"F1 = \", F1)\n",
    "print(\"LangChain Metric = \", langeval)\n",
    "print(\"Latency = \" , latency)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devrev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
